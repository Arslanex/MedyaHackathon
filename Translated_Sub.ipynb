{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Konfigrasyonlar"
      ],
      "metadata": {
        "id": "IcVfAWLiLyDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILENAME = \"demo.mp4\"\n",
        "KEY = \"key.json\"\n",
        "CURRENT_LANGUAGE = \"en\"\n",
        "TARGET_LANGUAGE = \"tr\""
      ],
      "metadata": {
        "id": "NEZGfANKL25w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gerekli Kütüphane Kurulumları\n"
      ],
      "metadata": {
        "id": "zCE1KWgrLlq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet srt_file_translator\n",
        "\n",
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "!pip install --quiet moviepy==2.0.0.dev2\n",
        "!pip install --quiet imageio==2.25.1\n",
        "!pip install --quiet ffmpeg-python==0.2.0\n",
        "!pip install --quiet faster-whisper==0.7.0\n",
        "!pip install --quiet python-docx"
      ],
      "metadata": {
        "id": "Tnfw1YAe7o-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install imagemagick\n",
        "!cat /etc/ImageMagick-6/policy.xml | sed 's/none/read,write/g'> /etc/ImageMagick-6/policy.xml"
      ],
      "metadata": {
        "id": "_bJQtXdQK6VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Functions"
      ],
      "metadata": {
        "id": "XGBQIxa_L3r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "import ffmpeg\n",
        "import json\n",
        "from docx import Document\n",
        "import re\n",
        "from srt_file_translator import Translator\n",
        "from moviepy.editor import TextClip, CompositeVideoClip, ColorClip\n",
        "import numpy as np\n",
        "from moviepy.editor import TextClip, CompositeVideoClip, concatenate_videoclips,VideoFileClip, ColorClip"
      ],
      "metadata": {
        "id": "Sdl2yp0SprlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV-x8rZHpWpX"
      },
      "outputs": [],
      "source": [
        "#\n",
        "def extract_sound_file(fileName):\n",
        "  audiofilename = fileName.replace(\".mp4\",'.mp3')\n",
        "\n",
        "  # Create the ffmpeg input strea m\n",
        "  input_stream = ffmpeg.input(fileName)\n",
        "  audio = input_stream.audio\n",
        "  output_stream = ffmpeg.output(audio, audiofilename)\n",
        "  output_stream = ffmpeg.overwrite_output(output_stream)\n",
        "\n",
        "  ffmpeg.run(output_stream)\n",
        "  return audiofilename\n",
        "\n",
        "#\n",
        "def load_model(model_size=\"medium\"):\n",
        "  return WhisperModel(model_size)\n",
        "\n",
        "#\n",
        "def create_segments(model, audiofilename):\n",
        "  segments, info = model.transcribe(audiofilename, word_timestamps=True)\n",
        "  return segments\n",
        "\n",
        "#\n",
        "def segments_to_srt(segments, output_filename=\"transcription.srt\"):\n",
        "    with open(output_filename, 'w', encoding='utf-8') as file:\n",
        "        for i, segment in enumerate(segments, start=1):\n",
        "            for word in segment.words:\n",
        "                start = word.start\n",
        "                end = word.end\n",
        "                # SRT formatında zamanı formatlama\n",
        "                start_srt = \"%02d:%02d:%02d,%03d\" % (int(start / 3600), int(start / 60 % 60), int(start % 60), int(start * 1000 % 1000))\n",
        "                end_srt = \"%02d:%02d:%02d,%03d\" % (int(end / 3600), int(end / 60 % 60), int(end % 60), int(end * 1000 % 1000))\n",
        "                file.write(f\"{i}\\n\")\n",
        "                file.write(f\"{start_srt} --> {end_srt}\\n\")\n",
        "                file.write(f\"{word.word}\\n\\n\")\n",
        "\n",
        "#\n",
        "def process(segments):\n",
        "  segments = list(segments)\n",
        "\n",
        "  segments_to_srt(segments)\n",
        "\n",
        "  wordlevel_info = []\n",
        "  for segment in segments:\n",
        "    for word in segment.words:\n",
        "      wordlevel_info.append({'word':word.word,'start':word.start,'end':word.end})\n",
        "\n",
        "  return wordlevel_info\n",
        "\n",
        "#\n",
        "def read_json(fileName='translated.json'):\n",
        "  with open(fileName, 'r') as f:\n",
        "    wordlevel_info_modified = json.load(f)\n",
        "\n",
        "  return wordlevel_info_modified\n",
        "\n",
        "#\n",
        "def split_text_into_lines(data):\n",
        "\n",
        "    MaxChars = 30\n",
        "    #maxduration in seconds\n",
        "    MaxDuration = 2.5\n",
        "    #Split if nothing is spoken (gap) for these many seconds\n",
        "    MaxGap = 1.5\n",
        "\n",
        "    subtitles = []\n",
        "    line = []\n",
        "    line_duration = 0\n",
        "    line_chars = 0\n",
        "\n",
        "\n",
        "    for idx,word_data in enumerate(data):\n",
        "        word = word_data[\"word\"]\n",
        "        start = word_data[\"start\"]\n",
        "        end = word_data[\"end\"]\n",
        "\n",
        "        line.append(word_data)\n",
        "        line_duration += end - start\n",
        "\n",
        "        temp = \" \".join(item[\"word\"] for item in line)\n",
        "\n",
        "\n",
        "        # Check if adding a new word exceeds the maximum character count or duration\n",
        "        new_line_chars = len(temp)\n",
        "\n",
        "        duration_exceeded = line_duration > MaxDuration\n",
        "        chars_exceeded = new_line_chars > MaxChars\n",
        "        if idx>0:\n",
        "          gap = word_data['start'] - data[idx-1]['end']\n",
        "          # print (word,start,end,gap)\n",
        "          maxgap_exceeded = gap > MaxGap\n",
        "        else:\n",
        "          maxgap_exceeded = False\n",
        "\n",
        "\n",
        "        if duration_exceeded or chars_exceeded or maxgap_exceeded:\n",
        "            if line:\n",
        "                subtitle_line = {\n",
        "                    \"word\": \" \".join(item[\"word\"] for item in line),\n",
        "                    \"start\": line[0][\"start\"],\n",
        "                    \"end\": line[-1][\"end\"],\n",
        "                    \"textcontents\": line\n",
        "                }\n",
        "                subtitles.append(subtitle_line)\n",
        "                line = []\n",
        "                line_duration = 0\n",
        "                line_chars = 0\n",
        "\n",
        "\n",
        "    if line:\n",
        "        subtitle_line = {\n",
        "            \"word\": \" \".join(item[\"word\"] for item in line),\n",
        "            \"start\": line[0][\"start\"],\n",
        "            \"end\": line[-1][\"end\"],\n",
        "            \"textcontents\": line\n",
        "        }\n",
        "        subtitles.append(subtitle_line)\n",
        "\n",
        "    return subtitles\n",
        "\n",
        "#\n",
        "def create_caption(textJSON, framesize,font = \"Helvetica\",color='white', highlight_color='yellow',stroke_color='black',stroke_width=1.5):\n",
        "    wordcount = len(textJSON['textcontents'])\n",
        "    full_duration = textJSON['end']-textJSON['start']\n",
        "\n",
        "    word_clips = []\n",
        "    xy_textclips_positions =[]\n",
        "\n",
        "    x_pos = 0\n",
        "    y_pos = 0\n",
        "    line_width = 0  # Total width of words in the current line\n",
        "    frame_width = framesize[0]\n",
        "    frame_height = framesize[1]\n",
        "\n",
        "    x_buffer = frame_width*1/10\n",
        "\n",
        "    max_line_width = frame_width - 2 * (x_buffer)\n",
        "\n",
        "    fontsize = int(frame_height * 0.075) #7.5 percent of video height\n",
        "\n",
        "    space_width = \"\"\n",
        "    space_height = \"\"\n",
        "\n",
        "    for index,wordJSON in enumerate(textJSON['textcontents']):\n",
        "      duration = wordJSON['end']-wordJSON['start']\n",
        "      word_clip = TextClip(wordJSON['word'], font = font,fontsize=fontsize, color=color,stroke_color=stroke_color,stroke_width=stroke_width).set_start(textJSON['start']).set_duration(full_duration)\n",
        "      word_clip_space = TextClip(\" \", font = font,fontsize=fontsize, color=color).set_start(textJSON['start']).set_duration(full_duration)\n",
        "      word_width, word_height = word_clip.size\n",
        "      space_width,space_height = word_clip_space.size\n",
        "      if line_width + word_width+ space_width <= max_line_width:\n",
        "            # Store info of each word_clip created\n",
        "            xy_textclips_positions.append({\n",
        "                \"x_pos\":x_pos,\n",
        "                \"y_pos\": y_pos,\n",
        "                \"width\" : word_width,\n",
        "                \"height\" : word_height,\n",
        "                \"word\": wordJSON['word'],\n",
        "                \"start\": wordJSON['start'],\n",
        "                \"end\": wordJSON['end'],\n",
        "                \"duration\": duration\n",
        "            })\n",
        "\n",
        "            word_clip = word_clip.set_position((x_pos, y_pos))\n",
        "            word_clip_space = word_clip_space.set_position((x_pos+ word_width, y_pos))\n",
        "\n",
        "            x_pos = x_pos + word_width+ space_width\n",
        "            line_width = line_width+ word_width + space_width\n",
        "      else:\n",
        "            # Move to the next line\n",
        "            x_pos = 0\n",
        "            y_pos = y_pos+ word_height+10\n",
        "            line_width = word_width + space_width\n",
        "\n",
        "            # Store info of each word_clip created\n",
        "            xy_textclips_positions.append({\n",
        "                \"x_pos\":x_pos,\n",
        "                \"y_pos\": y_pos,\n",
        "                \"width\" : word_width,\n",
        "                \"height\" : word_height,\n",
        "                \"word\": wordJSON['word'],\n",
        "                \"start\": wordJSON['start'],\n",
        "                \"end\": wordJSON['end'],\n",
        "                \"duration\": duration\n",
        "            })\n",
        "\n",
        "            word_clip = word_clip.set_position((x_pos, y_pos))\n",
        "            word_clip_space = word_clip_space.set_position((x_pos+ word_width , y_pos))\n",
        "            x_pos = word_width + space_width\n",
        "\n",
        "\n",
        "      word_clips.append(word_clip)\n",
        "      word_clips.append(word_clip_space)\n",
        "\n",
        "\n",
        "    for highlight_word in xy_textclips_positions:\n",
        "\n",
        "      word_clip_highlight = TextClip(highlight_word['word'], font = font,fontsize=fontsize, color=highlight_color,stroke_color=stroke_color,stroke_width=stroke_width).set_start(highlight_word['start']).set_duration(highlight_word['duration'])\n",
        "      word_clip_highlight = word_clip_highlight.set_position((highlight_word['x_pos'], highlight_word['y_pos']))\n",
        "      word_clips.append(word_clip_highlight)\n",
        "\n",
        "    return word_clips,xy_textclips_positions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def srt_to_docx(srt_file_path=\"transcription.srt\", docx_file_path=\"transcription.docx\"):\n",
        "    doc = Document()\n",
        "\n",
        "    full_text = ''\n",
        "\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        srt_content = file.read()\n",
        "\n",
        "    subtitles = re.split(r'\\n\\n+', srt_content)\n",
        "\n",
        "    for subtitle in subtitles:\n",
        "        lines = subtitle.split('\\n')[2:]\n",
        "        subtitle_text = ' '.join(lines)\n",
        "        full_text += subtitle_text + ' '\n",
        "\n",
        "    doc.add_paragraph(full_text)\n",
        "\n",
        "    doc.save(docx_file_path)\n",
        "\n",
        "#\n",
        "def parse_time_to_seconds(time_str):\n",
        "    \"\"\"SRT zaman formatını saniyeye çevirir.\"\"\"\n",
        "    hours, minutes, seconds, milliseconds = map(int, re.split('[:,]', time_str))\n",
        "    return hours * 3600 + minutes * 60 + seconds + milliseconds / 1000\n",
        "\n",
        "def srt_to_json(srt_file_path = 'translated.srt', json_file_path = 'translated.json'):\n",
        "    \"\"\"SRT dosyasını okuyup her kelime için JSON formatında kaydeder.\"\"\"\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        srt_content = file.read()\n",
        "\n",
        "    entries = []\n",
        "    for block in re.split(r'\\n\\n+', srt_content.strip()):\n",
        "        lines = block.split('\\n')\n",
        "        if len(lines) >= 3:\n",
        "            time_range = lines[1]\n",
        "            text_lines = lines[2:]\n",
        "            start_time_str, end_time_str = re.findall(r'(\\d{2}:\\d{2}:\\d{2},\\d{3})', time_range)\n",
        "            start_time = parse_time_to_seconds(start_time_str)\n",
        "            end_time = parse_time_to_seconds(end_time_str)\n",
        "            text = ' '.join(text_lines)\n",
        "\n",
        "            words = text.split()\n",
        "            duration = (end_time - start_time) / max(len(words), 1)\n",
        "            for i, word in enumerate(words):\n",
        "                word_start_time = start_time + i * duration\n",
        "                word_end_time = word_start_time + duration\n",
        "                entries.append({\n",
        "                    'word': word,\n",
        "                    'start': round(word_start_time, 2),\n",
        "                    'end': round(word_end_time, 2)\n",
        "                })\n",
        "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(entries, json_file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "hH625-zR1aCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def find_word_in_srt(search_word, srt_file_path=\"transcription.srt\"):\n",
        "    search_word = search_word.lower()\n",
        "    matches = []\n",
        "\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line_lower = line.lower()\n",
        "            if re.search(r'\\b' + re.escape(search_word) + r'\\b', line_lower):\n",
        "                matches.append(line.strip())\n",
        "\n",
        "    return matches\n",
        "\n",
        "#\n",
        "def find_word_and_timestamp_in_srt(search_word, srt_file_path=\"transcription.srt\"):\n",
        "    search_word = search_word.lower()\n",
        "    results = []\n",
        "\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    subtitles = re.split(r'\\n\\n+', content)\n",
        "\n",
        "    for subtitle in subtitles:\n",
        "        lines = subtitle.split('\\n')\n",
        "        if len(lines) < 3:\n",
        "            continue\n",
        "\n",
        "        time_info = lines[1]\n",
        "        text = ' '.join(lines[2:]).lower()\n",
        "\n",
        "        if re.search(r'\\b' + re.escape(search_word) + r'\\b', text):\n",
        "            results.append((time_info, '\\n'.join(lines[2:])))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "crGrFS2t11Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime"
      ],
      "metadata": {
        "id": "TdtGI-_RNGze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soundFile = extract_sound_file(FILENAME)"
      ],
      "metadata": {
        "id": "--i56hATNgHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model()"
      ],
      "metadata": {
        "id": "wjIItF4yNgSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = create_segments(model, soundFile)"
      ],
      "metadata": {
        "id": "UkjSJVTzq2Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wli = process(segments)"
      ],
      "metadata": {
        "id": "0fDKd-L9rJ41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(key_path=KEY)\n",
        "\n",
        "translator.srt_file_translator(\n",
        "    source_file=\"transcription.srt\",\n",
        "    target_file=\"translated.srt\",\n",
        "    source_language=CURRENT_LANGUAGE,\n",
        "    target_language=TARGET_LANGUAGE\n",
        ")"
      ],
      "metadata": {
        "id": "vt8nLBl58Axe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_json()"
      ],
      "metadata": {
        "id": "dibMYKi89KTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subtitles = split_text_into_lines(data)"
      ],
      "metadata": {
        "id": "fVF9Msaorf-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in subtitles:\n",
        "  json_str = json.dumps(line, indent=4)\n",
        "  print(json_str)"
      ],
      "metadata": {
        "id": "AeLy_bgqN6Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8I7n1-h7xFR"
      },
      "outputs": [],
      "source": [
        "input_video = VideoFileClip(FILENAME)\n",
        "frame_size = input_video.size\n",
        "\n",
        "all_linelevel_splits=[]\n",
        "\n",
        "for line in subtitles:\n",
        "  out_clips,positions = create_caption(line,frame_size)\n",
        "\n",
        "  max_width = 0\n",
        "  max_height = 0\n",
        "\n",
        "  for position in positions:\n",
        "    # print (out_clip.pos)\n",
        "    # break\n",
        "    x_pos, y_pos = position['x_pos'],position['y_pos']\n",
        "    width, height = position['width'],position['height']\n",
        "\n",
        "    max_width = max(max_width, x_pos + width)\n",
        "    max_height = max(max_height, y_pos + height)\n",
        "\n",
        "  color_clip = ColorClip(size=(int(max_width*1.1), int(max_height*1.1)),\n",
        "                       color=(64, 64, 64))\n",
        "  color_clip = color_clip.set_opacity(.6)\n",
        "  color_clip = color_clip.set_start(line['start']).set_duration(line['end']-line['start'])\n",
        "\n",
        "  # centered_clips = [each.set_position('center') for each in out_clips]\n",
        "\n",
        "  clip_to_overlay = CompositeVideoClip([color_clip]+ out_clips)\n",
        "  clip_to_overlay = clip_to_overlay.set_position(\"bottom\")\n",
        "\n",
        "\n",
        "  all_linelevel_splits.append(clip_to_overlay)\n",
        "\n",
        "input_video_duration = input_video.duration\n",
        "\n",
        "\n",
        "final_video = CompositeVideoClip([input_video] + all_linelevel_splits)\n",
        "\n",
        "# Set the audio of the final video to be the same as the input video\n",
        "final_video = final_video.set_audio(input_video.audio)\n",
        "\n",
        "# Save the final clip as a video file with the audio included\n",
        "final_video.write_videofile(\"output.mp4\", fps=24, codec=\"libx264\", audio_codec=\"aac\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ekstra Özellikler"
      ],
      "metadata": {
        "id": "EYiFuiX3OPkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "srt_to_docx()"
      ],
      "metadata": {
        "id": "tGsTY0ZTOidJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_word = \"pentagon\"\n",
        "matches = find_word_in_srt(search_word)\n",
        "\n",
        "if matches:\n",
        "    print(f\"'{search_word}' kelimesinin bulunduğu satırlar:\")\n",
        "    for match in matches:\n",
        "        print(match)\n",
        "else:\n",
        "    print(f\"'{search_word}' kelimesi bulunamadı.\")"
      ],
      "metadata": {
        "id": "1FcFDdQiOnQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_word = 'pentagon'  # Aranan kelime\n",
        "matches = find_word_and_timestamp_in_srt(search_word)\n",
        "\n",
        "if matches:\n",
        "    print(f\"'{search_word}' kelimesinin bulunduğu zamanlar ve satırlar:\")\n",
        "    for time_info, match in matches:\n",
        "        print(f\"Zaman: {time_info}\")\n",
        "        print(f\"Metin: {match}\\n\")\n",
        "else:\n",
        "    print(f\"'{search_word}' kelimesi bulunamadı.\")"
      ],
      "metadata": {
        "id": "pErc3WXhOpfE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}