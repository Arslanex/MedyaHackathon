{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Konfigrasyon"
      ],
      "metadata": {
        "id": "DNWMVAosQ1OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILENAME = \"demo.mp4\""
      ],
      "metadata": {
        "id": "kj5CpQP1Q1Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gerekli Kütüphane ve Program Kurulumları"
      ],
      "metadata": {
        "id": "S8L1TtZtQ04n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet srt_file_translator\n",
        "\n",
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "!pip install --quiet moviepy==2.0.0.dev2\n",
        "!pip install --quiet imageio==2.25.1\n",
        "!pip install --quiet ffmpeg-python==0.2.0\n",
        "!pip install --quiet faster-whisper==0.7.0\n",
        "!pip install --quiet python-docx"
      ],
      "metadata": {
        "id": "uHxyAG6Lro45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install  imagemagick\n",
        "!cat /etc/ImageMagick-6/policy.xml | sed 's/none/read,write/g'> /etc/ImageMagick-6/policy.xml"
      ],
      "metadata": {
        "id": "tjeNC8wjP3wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Functions\n"
      ],
      "metadata": {
        "id": "mb82AYKHqsie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "import ffmpeg\n",
        "import json\n",
        "from docx import Document\n",
        "import re\n",
        "from moviepy.editor import TextClip, CompositeVideoClip, concatenate_videoclips,VideoFileClip, ColorClip\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Sdl2yp0SprlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV-x8rZHpWpX"
      },
      "outputs": [],
      "source": [
        "#\n",
        "def extract_sound_file(fileName):\n",
        "  audiofilename = fileName.replace(\".mp4\",'.mp3')\n",
        "\n",
        "  # Create the ffmpeg input strea m\n",
        "  input_stream = ffmpeg.input(fileName)\n",
        "  audio = input_stream.audio\n",
        "  output_stream = ffmpeg.output(audio, audiofilename)\n",
        "  output_stream = ffmpeg.overwrite_output(output_stream)\n",
        "\n",
        "  ffmpeg.run(output_stream)\n",
        "  return audiofilename\n",
        "\n",
        "#\n",
        "def load_model(model_size=\"medium\"):\n",
        "  return WhisperModel(model_size)\n",
        "\n",
        "#\n",
        "def create_segments(model, audiofilename):\n",
        "  segments, info = model.transcribe(audiofilename, word_timestamps=True)\n",
        "  return segments\n",
        "\n",
        "#\n",
        "def print_segments(segments):\n",
        "  segments = list(segments)\n",
        "  for segment in segments:\n",
        "    for word in segment.words:\n",
        "        print(\"[%.2fs -> %.2fs] %s\" % (word.start, word.end, word.word))\n",
        "\n",
        "#\n",
        "def process(segments):\n",
        "  segments = list(segments)\n",
        "\n",
        "  wordlevel_info = []\n",
        "  for segment in segments:\n",
        "    for word in segment.words:\n",
        "      wordlevel_info.append({'word':word.word,'start':word.start,'end':word.end})\n",
        "\n",
        "  return wordlevel_info\n",
        "\n",
        "#\n",
        "def dump_json(data):\n",
        "  with open('data.json', 'w') as f:\n",
        "    json.dump(data, f,indent=4)\n",
        "\n",
        "#\n",
        "def read_json(fileName='data.json'):\n",
        "  with open(fileName, 'r') as f:\n",
        "    wordlevel_info_modified = json.load(f)\n",
        "\n",
        "  return wordlevel_info_modified\n",
        "\n",
        "#\n",
        "def split_text_into_lines(data):\n",
        "\n",
        "    MaxChars = 30\n",
        "    #maxduration in seconds\n",
        "    MaxDuration = 2.5\n",
        "    #Split if nothing is spoken (gap) for these many seconds\n",
        "    MaxGap = 1.5\n",
        "\n",
        "    subtitles = []\n",
        "    line = []\n",
        "    line_duration = 0\n",
        "    line_chars = 0\n",
        "\n",
        "\n",
        "    for idx,word_data in enumerate(data):\n",
        "        word = word_data[\"word\"]\n",
        "        start = word_data[\"start\"]\n",
        "        end = word_data[\"end\"]\n",
        "\n",
        "        line.append(word_data)\n",
        "        line_duration += end - start\n",
        "\n",
        "        temp = \" \".join(item[\"word\"] for item in line)\n",
        "\n",
        "\n",
        "        # Check if adding a new word exceeds the maximum character count or duration\n",
        "        new_line_chars = len(temp)\n",
        "\n",
        "        duration_exceeded = line_duration > MaxDuration\n",
        "        chars_exceeded = new_line_chars > MaxChars\n",
        "        if idx>0:\n",
        "          gap = word_data['start'] - data[idx-1]['end']\n",
        "          # print (word,start,end,gap)\n",
        "          maxgap_exceeded = gap > MaxGap\n",
        "        else:\n",
        "          maxgap_exceeded = False\n",
        "\n",
        "\n",
        "        if duration_exceeded or chars_exceeded or maxgap_exceeded:\n",
        "            if line:\n",
        "                subtitle_line = {\n",
        "                    \"word\": \" \".join(item[\"word\"] for item in line),\n",
        "                    \"start\": line[0][\"start\"],\n",
        "                    \"end\": line[-1][\"end\"],\n",
        "                    \"textcontents\": line\n",
        "                }\n",
        "                subtitles.append(subtitle_line)\n",
        "                line = []\n",
        "                line_duration = 0\n",
        "                line_chars = 0\n",
        "\n",
        "\n",
        "    if line:\n",
        "        subtitle_line = {\n",
        "            \"word\": \" \".join(item[\"word\"] for item in line),\n",
        "            \"start\": line[0][\"start\"],\n",
        "            \"end\": line[-1][\"end\"],\n",
        "            \"textcontents\": line\n",
        "        }\n",
        "        subtitles.append(subtitle_line)\n",
        "\n",
        "    return subtitles\n",
        "\n",
        "#\n",
        "def create_caption(textJSON, framesize,font = \"Helvetica\",color='white', highlight_color='yellow',stroke_color='black',stroke_width=1.5):\n",
        "    wordcount = len(textJSON['textcontents'])\n",
        "    full_duration = textJSON['end']-textJSON['start']\n",
        "\n",
        "    word_clips = []\n",
        "    xy_textclips_positions =[]\n",
        "\n",
        "    x_pos = 0\n",
        "    y_pos = 0\n",
        "    line_width = 0  # Total width of words in the current line\n",
        "    frame_width = framesize[0]\n",
        "    frame_height = framesize[1]\n",
        "\n",
        "    x_buffer = frame_width*1/10\n",
        "\n",
        "    max_line_width = frame_width - 2 * (x_buffer)\n",
        "\n",
        "    fontsize = int(frame_height * 0.075) #7.5 percent of video height\n",
        "\n",
        "    space_width = \"\"\n",
        "    space_height = \"\"\n",
        "\n",
        "    for index,wordJSON in enumerate(textJSON['textcontents']):\n",
        "      duration = wordJSON['end']-wordJSON['start']\n",
        "      word_clip = TextClip(wordJSON['word'], font = font,fontsize=fontsize, color=color,stroke_color=stroke_color,stroke_width=stroke_width).set_start(textJSON['start']).set_duration(full_duration)\n",
        "      word_clip_space = TextClip(\" \", font = font,fontsize=fontsize, color=color).set_start(textJSON['start']).set_duration(full_duration)\n",
        "      word_width, word_height = word_clip.size\n",
        "      space_width,space_height = word_clip_space.size\n",
        "      if line_width + word_width+ space_width <= max_line_width:\n",
        "            # Store info of each word_clip created\n",
        "            xy_textclips_positions.append({\n",
        "                \"x_pos\":x_pos,\n",
        "                \"y_pos\": y_pos,\n",
        "                \"width\" : word_width,\n",
        "                \"height\" : word_height,\n",
        "                \"word\": wordJSON['word'],\n",
        "                \"start\": wordJSON['start'],\n",
        "                \"end\": wordJSON['end'],\n",
        "                \"duration\": duration\n",
        "            })\n",
        "\n",
        "            word_clip = word_clip.set_position((x_pos, y_pos))\n",
        "            word_clip_space = word_clip_space.set_position((x_pos+ word_width, y_pos))\n",
        "\n",
        "            x_pos = x_pos + word_width+ space_width\n",
        "            line_width = line_width+ word_width + space_width\n",
        "      else:\n",
        "            # Move to the next line\n",
        "            x_pos = 0\n",
        "            y_pos = y_pos+ word_height+10\n",
        "            line_width = word_width + space_width\n",
        "\n",
        "            # Store info of each word_clip created\n",
        "            xy_textclips_positions.append({\n",
        "                \"x_pos\":x_pos,\n",
        "                \"y_pos\": y_pos,\n",
        "                \"width\" : word_width,\n",
        "                \"height\" : word_height,\n",
        "                \"word\": wordJSON['word'],\n",
        "                \"start\": wordJSON['start'],\n",
        "                \"end\": wordJSON['end'],\n",
        "                \"duration\": duration\n",
        "            })\n",
        "\n",
        "            word_clip = word_clip.set_position((x_pos, y_pos))\n",
        "            word_clip_space = word_clip_space.set_position((x_pos+ word_width , y_pos))\n",
        "            x_pos = word_width + space_width\n",
        "\n",
        "\n",
        "      word_clips.append(word_clip)\n",
        "      word_clips.append(word_clip_space)\n",
        "\n",
        "\n",
        "    for highlight_word in xy_textclips_positions:\n",
        "\n",
        "      word_clip_highlight = TextClip(highlight_word['word'], font = font,fontsize=fontsize, color=highlight_color,stroke_color=stroke_color,stroke_width=stroke_width).set_start(highlight_word['start']).set_duration(highlight_word['duration'])\n",
        "      word_clip_highlight = word_clip_highlight.set_position((highlight_word['x_pos'], highlight_word['y_pos']))\n",
        "      word_clips.append(word_clip_highlight)\n",
        "\n",
        "    return word_clips,xy_textclips_positions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dönüşüm Fonksiyonları"
      ],
      "metadata": {
        "id": "azmesMZaRLzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def segments_to_srt(segments, output_filename):\n",
        "    with open(output_filename, 'w', encoding='utf-8') as file:\n",
        "        for i, segment in enumerate(segments, start=1):\n",
        "            for word in segment.words:\n",
        "                start = word.start\n",
        "                end = word.end\n",
        "                # SRT formatında zamanı formatlama\n",
        "                start_srt = \"%02d:%02d:%02d,%03d\" % (int(start / 3600), int(start / 60 % 60), int(start % 60), int(start * 1000 % 1000))\n",
        "                end_srt = \"%02d:%02d:%02d,%03d\" % (int(end / 3600), int(end / 60 % 60), int(end % 60), int(end * 1000 % 1000))\n",
        "                file.write(f\"{i}\\n\")\n",
        "                file.write(f\"{start_srt} --> {end_srt}\\n\")\n",
        "                file.write(f\"{word.word}\\n\\n\")\n",
        "\n",
        "#\n",
        "def srt_to_docx(srt_file_path=\"transcription.srt\", docx_file_path=\"transcription.docx\"):\n",
        "    doc = Document()\n",
        "\n",
        "    full_text = ''\n",
        "\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        srt_content = file.read()\n",
        "\n",
        "    subtitles = re.split(r'\\n\\n+', srt_content)\n",
        "\n",
        "    for subtitle in subtitles:\n",
        "        lines = subtitle.split('\\n')[2:]\n",
        "        subtitle_text = ' '.join(lines)\n",
        "        full_text += subtitle_text + ' '\n",
        "\n",
        "    doc.add_paragraph(full_text)\n",
        "\n",
        "    doc.save(docx_file_path)\n",
        "\n",
        "#\n",
        "def seconds_to_srt_time(seconds):\n",
        "    \"\"\"Saniye cinsinden zamanı saat:dakika:saniye,milisaniye formatına çevir.\"\"\"\n",
        "    ms = int((seconds - int(seconds)) * 1000)\n",
        "    h = int(seconds // 3600)\n",
        "    m = int((seconds % 3600) // 60)\n",
        "    s = int(seconds % 60)\n",
        "    return f\"{h:02}:{m:02}:{s:02},{ms:03}\"\n",
        "\n",
        "def format_to_srt(data):\n",
        "    srt_content = \"\"\n",
        "    for index, item in enumerate(data, start=1):\n",
        "        start_time = seconds_to_srt_time(item['start'])\n",
        "        end_time = seconds_to_srt_time(item['end'])\n",
        "        word = item['word'].strip()\n",
        "        srt_content += f\"{index}\\n{start_time} --> {end_time}\\n{word}\\n\\n\"\n",
        "    return srt_content\n",
        "\n",
        "def json_to_srt(json_file=\"data.json\", output_filename=\"transcription.srt\"):\n",
        "  with open(json_file, 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  srt_content = format_to_srt(data)\n",
        "\n",
        "  with open(output_filename, 'w') as srt_file:\n",
        "      srt_file.write(srt_content)"
      ],
      "metadata": {
        "id": "uTnBqBSM07oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "DfZbBWVRROgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def find_word_in_srt(search_word, srt_file_path=\"transcription.srt\"):\n",
        "    search_word = search_word.lower()\n",
        "    matches = []\n",
        "\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line_lower = line.lower()\n",
        "            if re.search(r'\\b' + re.escape(search_word) + r'\\b', line_lower):\n",
        "                matches.append(line.strip())\n",
        "\n",
        "    return matches\n",
        "\n",
        "#\n",
        "def find_word_and_timestamp_in_srt(search_word, srt_file_path=\"transcription.srt\"):\n",
        "    search_word = search_word.lower()\n",
        "    results = []\n",
        "\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    subtitles = re.split(r'\\n\\n+', content)\n",
        "\n",
        "    for subtitle in subtitles:\n",
        "        lines = subtitle.split('\\n')\n",
        "        if len(lines) < 3:\n",
        "            continue\n",
        "\n",
        "        time_info = lines[1]\n",
        "        text = ' '.join(lines[2:]).lower()\n",
        "\n",
        "        if re.search(r'\\b' + re.escape(search_word) + r'\\b', text):\n",
        "            results.append((time_info, '\\n'.join(lines[2:])))\n",
        "\n",
        "    return results\n",
        "\n",
        "#\n",
        "def time_to_seconds(time_str):\n",
        "    \"\"\"Zaman damgasını saniye cinsinden döndür.\"\"\"\n",
        "    hours, minutes, seconds = [int(part) for part in time_str.split(':')[0:3]]\n",
        "    seconds += 60 * minutes + 3600 * hours\n",
        "    return seconds\n",
        "\n",
        "#\n",
        "def find_word_and_seconds_in_srt(search_word, srt_file_path=\"transcription.srt\"):\n",
        "    search_word = search_word.lower()\n",
        "    seconds_list = []\n",
        "\n",
        "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    subtitles = re.split(r'\\n\\n+', content)\n",
        "\n",
        "    for subtitle in subtitles:\n",
        "        lines = subtitle.split('\\n')\n",
        "        if len(lines) < 3:\n",
        "            continue\n",
        "\n",
        "        time_info = lines[1]\n",
        "        text = ' '.join(lines[2:]).lower()\n",
        "\n",
        "        if re.search(r'\\b' + re.escape(search_word) + r'\\b', text):\n",
        "            start_time_str = time_info.split(' --> ')[0]\n",
        "            start_seconds = time_to_seconds(start_time_str.split(',')[0])\n",
        "            seconds_list.append(start_seconds)\n",
        "\n",
        "    return seconds_list"
      ],
      "metadata": {
        "id": "crGrFS2t11Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime"
      ],
      "metadata": {
        "id": "UEZWh8EPRRJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soundFile = extract_sound_file(FILENAME)"
      ],
      "metadata": {
        "id": "7vvlbQJmRVTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model()"
      ],
      "metadata": {
        "id": "UkjSJVTzq2Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = create_segments(model, soundFile)"
      ],
      "metadata": {
        "id": "CJQ_56BGRXQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wli = process(segments)"
      ],
      "metadata": {
        "id": "tKkZ7QToRYAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_json(wli)"
      ],
      "metadata": {
        "id": "0fDKd-L9rJ41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_json()"
      ],
      "metadata": {
        "id": "Juk-vcxGRaRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subtitles = split_text_into_lines(data)"
      ],
      "metadata": {
        "id": "fVF9Msaorf-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in subtitles:\n",
        "  json_str = json.dumps(line, indent=4)\n",
        "  print(json_str)"
      ],
      "metadata": {
        "id": "IrfTTHPYRb1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8I7n1-h7xFR"
      },
      "outputs": [],
      "source": [
        "input_video = VideoFileClip(FILENAME)\n",
        "frame_size = input_video.size\n",
        "\n",
        "all_linelevel_splits=[]\n",
        "\n",
        "for line in subtitles:\n",
        "  out_clips,positions = create_caption(line,frame_size)\n",
        "\n",
        "  max_width = 0\n",
        "  max_height = 0\n",
        "\n",
        "  for position in positions:\n",
        "    # print (out_clip.pos)\n",
        "    # break\n",
        "    x_pos, y_pos = position['x_pos'],position['y_pos']\n",
        "    width, height = position['width'],position['height']\n",
        "\n",
        "    max_width = max(max_width, x_pos + width)\n",
        "    max_height = max(max_height, y_pos + height)\n",
        "\n",
        "  color_clip = ColorClip(size=(int(max_width*1.1), int(max_height*1.1)),\n",
        "                       color=(64, 64, 64))\n",
        "  color_clip = color_clip.set_opacity(.6)\n",
        "  color_clip = color_clip.set_start(line['start']).set_duration(line['end']-line['start'])\n",
        "\n",
        "  # centered_clips = [each.set_position('center') for each in out_clips]\n",
        "\n",
        "  clip_to_overlay = CompositeVideoClip([color_clip]+ out_clips)\n",
        "  clip_to_overlay = clip_to_overlay.set_position(\"bottom\")\n",
        "\n",
        "\n",
        "  all_linelevel_splits.append(clip_to_overlay)\n",
        "\n",
        "input_video_duration = input_video.duration\n",
        "\n",
        "\n",
        "final_video = CompositeVideoClip([input_video] + all_linelevel_splits)\n",
        "\n",
        "# Set the audio of the final video to be the same as the input video\n",
        "final_video = final_video.set_audio(input_video.audio)\n",
        "\n",
        "# Save the final clip as a video file with the audio included\n",
        "final_video.write_videofile(\"output.mp4\", fps=24, codec=\"libx264\", audio_codec=\"aac\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Features"
      ],
      "metadata": {
        "id": "a-jel6gP0uVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_srt(segments, \"transcription.srt\")"
      ],
      "metadata": {
        "id": "MH0p1hAu0yuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_to_srt()"
      ],
      "metadata": {
        "id": "dGF_ivPH4Im8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SRT to Docs"
      ],
      "metadata": {
        "id": "WgjMYwZt1otc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "srt_to_docx()"
      ],
      "metadata": {
        "id": "zZZOKZQ-1rPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find"
      ],
      "metadata": {
        "id": "e4xeri4h1vdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_word = \"pentagon\"\n",
        "matches = find_word_in_srt(search_word)\n",
        "\n",
        "if matches:\n",
        "    print(f\"'{search_word}' kelimesinin bulunduğu satırlar:\")\n",
        "    for match in matches:\n",
        "        print(match)\n",
        "else:\n",
        "    print(f\"'{search_word}' kelimesi bulunamadı.\")"
      ],
      "metadata": {
        "id": "V8UOTvJB2Gxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_word = 'pentagon'  # Aranan kelime\n",
        "matches = find_word_and_timestamp_in_srt(search_word)\n",
        "\n",
        "if matches:\n",
        "    print(f\"'{search_word}' kelimesinin bulunduğu zamanlar ve satırlar:\")\n",
        "    for time_info, match in matches:\n",
        "        print(f\"Zaman: {time_info}\")\n",
        "        print(f\"Metin: {match}\\n\")\n",
        "else:\n",
        "    print(f\"'{search_word}' kelimesi bulunamadı.\")"
      ],
      "metadata": {
        "id": "NLQXoINZ3Gri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_word = 'pentagon'\n",
        "seconds_list = find_word_and_seconds_in_srt(search_word)\n",
        "\n",
        "if seconds_list:\n",
        "    print(f\"'{search_word}' kelimesinin bulunduğu saniyeler:\")\n",
        "    for seconds in seconds_list:\n",
        "        print(seconds)\n",
        "else:\n",
        "    print(f\"'{search_word}' kelimesi bulunamadı.\")"
      ],
      "metadata": {
        "id": "hukcTGdG3G0A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
